{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LPnFv92NRwq",
        "outputId": "94d9f7ef-8f5d-4a6b-9fd3-6e8076e028bf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "c:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\paramiko\\pkey.py:82: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
            "  \"cipher\": algorithms.TripleDES,\n",
            "c:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.Blowfish and will be removed from this module in 45.0.0.\n",
            "  \"class\": algorithms.Blowfish,\n",
            "c:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:243: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
            "  \"class\": algorithms.TripleDES,\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6RrZA64BNoGk"
      },
      "outputs": [],
      "source": [
        "train_df=pd.read_csv(r'C:\\Users\\Admin\\Desktop\\train.csv')\n",
        "test_df=pd.read_csv(r'C:\\Users\\Admin\\Desktop\\test.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "DNFEQc0INSlF",
        "outputId": "d1e222bb-e048-4b64-cc68-baa70b5edc8f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword location                                               text  \\\n",
              "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
              "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
              "\n",
              "   target  \n",
              "0       1  \n",
              "1       1  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "__vJDeXyPT0Q",
        "outputId": "7114d7fd-144b-4ed8-8179-6f6a62689bb2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "id             0\n",
              "keyword       61\n",
              "location    2533\n",
              "text           0\n",
              "target         0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2yblvZFEPTxS"
      },
      "outputs": [],
      "source": [
        "train_df['keyword']=train_df['keyword'].fillna('None')\n",
        "train_df['location']=train_df['location'].fillna('Unknown')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0ez3KaSSCTB",
        "outputId": "bf876c0b-1cbd-4596-b488-b9c6969f28b4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9Vi4MBV-NZwN"
      },
      "outputs": [],
      "source": [
        "# Function to clean text\n",
        "def clean_text(text):\n",
        "    text= text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # Remove URLs\n",
        "    text = re.sub(r'\\@\\w+|\\#', '', text)  # Remove mentions and hashtags\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove special characters and numbers\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-AmDs3cARgbz"
      },
      "outputs": [],
      "source": [
        "# Clean the text\n",
        "train_df['cleaned_text'] = train_df['text'].apply(clean_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abEoSAgCRoQA",
        "outputId": "04452aca-47ed-414c-cf98-d1259915320e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "# ... your existing code ...\n",
        "\n",
        "# Download 'punkt_tab' before using word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Function to remove stopwords\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text)\n",
        "    return ' '.join([word for word in words if word not in stop_words])\n",
        "\n",
        "# ... your existing code ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "zaGzipFNR9PS"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Function for stemming\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import LancasterStemmer\n",
        "nltk.download('punkt')\n",
        "stemmer = LancasterStemmer()\n",
        "def stem_text(text):\n",
        "   words = word_tokenize(text)\n",
        "   return ' '.join([stemmer.stem(word) for word in words])\n",
        "\n",
        "train_df['cleaned_text'] = train_df['cleaned_text'].apply(stem_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "NE4tl-5vTAsw"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Function for lemmatization\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatize_text(text):\n",
        "   words = word_tokenize(text)\n",
        "   return ' '.join([lemmatizer.lemmatize(word) for word in words])\n",
        "\n",
        "# Apply lemmatization\n",
        "train_df['cleaned_text'] = train_df['cleaned_text'].apply(lemmatize_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Q6j1hpC4TNeF"
      },
      "outputs": [],
      "source": [
        "# Tokenize the cleaned text for Word2Vec\n",
        "tokenized_text = [word_tokenize(text) for text in train_df['cleaned_text']]\n",
        "\n",
        "# Train Word2Vec model\n",
        "word2vec_model = Word2Vec(sentences=tokenized_text, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Save the model for future use\n",
        "word2vec_model.save(\"word2vec.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "zZhqbhCeTT9_"
      },
      "outputs": [],
      "source": [
        "# Function to get average Word2Vec vectors for each text\n",
        "def get_vector(text):\n",
        "    words = word_tokenize(text)\n",
        "    vector = np.zeros(100)  # Assuming vector_size=100\n",
        "    count = 0\n",
        "    for word in words:\n",
        "        if word in word2vec_model.wv:\n",
        "            vector += word2vec_model.wv[word]\n",
        "            count += 1\n",
        "    if count > 0:\n",
        "        vector /= count\n",
        "    return vector\n",
        "\n",
        "# Apply the function to get vectors\n",
        "train_df['text_vector'] = train_df['cleaned_text'].apply(get_vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxxlmAWMTYrg",
        "outputId": "ddbb42ed-1d4a-4db9-ff21-214271885ebe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7202889034799738\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Prepare features and labels\n",
        "X = np.array(train_df['text_vector'].tolist())\n",
        "y = train_df['target']\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a model\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = model.score(X_test, y_test)\n",
        "print(f'Accuracy: {accuracy}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KItO4wS2TeS4",
        "outputId": "6252dc14-38cc-4658-a8a9-7f9d1d17ae13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6940249507550886\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "\n",
        "# Train a model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = model.score(X_test, y_test)\n",
        "print(f'Accuracy: {accuracy}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6eq0pIXT8St",
        "outputId": "bf9a5170-22cc-45c4-9901-fd14cf2d12fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.582403151674327\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
        "from sklearn.preprocessing import MinMaxScaler  # Import MinMaxScaler for scaling\n",
        "from sklearn.pipeline import Pipeline # Import the Pipeline class\n",
        "\n",
        "# ... your existing code ...\n",
        "\n",
        "# Train a model with scaling\n",
        "# Create a pipeline to first scale the data and then apply MultinomialNB\n",
        "model = Pipeline([('scaler', MinMaxScaler()), ('clf', MultinomialNB())])\n",
        "model.fit(X_train, y_train)  # Train the model\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = model.score(X_test, y_test)\n",
        "print(f'Accuracy: {accuracy}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wNV2ZyVT_8z"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
